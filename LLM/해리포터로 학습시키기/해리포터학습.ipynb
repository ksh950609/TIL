{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNj3+cFVsw1ES++/3tRxgDn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_i2rP2g0vbft"},"outputs":[],"source":["import os\n","import re\n","\n","# KaggleHub 다운로드된 데이터셋 경로\n","dataset_path = \"/root/.cache/kagglehub/datasets/shubhammaindola/harry-potter-books/versions/1\"\n","\n","def clean_text(filename):\n","    full_path = os.path.join(dataset_path, filename)  # 전체 경로 사용\n","\n","    if not os.path.exists(full_path):\n","        print(f\"파일을 찾을 수 없습니다: {full_path}\")\n","        return\n","\n","    with open(full_path, 'r', encoding='utf-8') as file:\n","        book_text = file.read()\n","# 클린텍스트로 만드는 이유는 한줄로 바꾼 것\n","    cleaned_text = re.sub(r'\\n+', ' ', book_text)  # 줄바꿈 제거\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # 여러 공백 제거\n","\n","    print(\"cleaned_\" + filename, len(cleaned_text), \"characters\")\n","\n","    with open(\"cleaned_\" + filename, 'w', encoding='utf-8') as file:\n","        file.write(cleaned_text)\n","\n","filenames_list = [\"02 Harry Potter and the Chamber of Secrets.txt\"]\n","\n","for filename in filenames_list:\n","    clean_text(filename)\n"]},{"cell_type":"code","source":["# 토큰화를 진행한다.\n","# tiktoken 다운로드 하고 진행\n","!pip install tiktoken\n","import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","text = \"Harry Potter was a wizard.\"\n","\n","tokens = tokenizer.encode(text)\n","\n","print(\"글자수: \", len(text), \"토큰수:\", len(tokens))\n","print(tokens)\n","print(tokenizer.decode(tokens))\n","for t in tokens:\n","    print(f\"{t}\\t -> {tokenizer.decode([t])}\")"],"metadata":{"id":"3fhjf5vnxgZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_SgsbpSDF38m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#데이터 로드\n","!pip install torch\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class MyDataset(Dataset):\n","    def __init__(self, txt, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n","        token_ids = tokenizer.encode(txt)\n","\n","        print(\"# of tokens in txt:\", len(token_ids))\n","\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            #\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            #뉴럴에 input하면\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            #타켓하면 비슷한 답변을 하게\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","# with open(\"cleaned_한글문서.txt\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n","with open(\"cleaned_02 Harry Potter and the Chamber of Secrets.txt\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n","    txt = file.read()\n","\n","dataset = MyDataset(txt, max_length = 32, stride = 4)\n","# drop 트루로 하면 랜덤으로 나옴\n","train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n"],"metadata":{"id":"AcPIY8tN73qS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataiter = iter(train_loader)\n","\n","x, y = next(dataiter)\n","\n","print(tokenizer.decode(x[0].tolist()))\n","print(tokenizer.decode(y[0].tolist()))"],"metadata":{"id":"JgewnPlv-JCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for char in text:\n","#    token_ids = tokenizer.encode(char)\n","#    decoded = tokenizer.decode(token_ids)\n","#    print(f\"{char} -> {token_ids} -> {decoded}\")"],"metadata":{"id":"WHjK17Mx7RZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델을 정의할 때 사용하는 상수들\n","\n","VOCAB_SIZE = tokenizer.n_vocab # 50257 Tiktoken\n","#VOCAB_SIZE = len(tokenizer) # AutoTokenizer\n","CONTEXT_LENGTH = 128  # Shortened context length (orig: 1024)\n","EMB_DIM = 768  # Embedding dimension\n","NUM_HEADS = 12  # Number of attention heads\n","NUM_LAYERS = 12  # Number of layers\n","DROP_RATE = 0.1  # Dropout rate\n","QKV_BIAS = False  # Query-key-value bias"],"metadata":{"id":"SB82VK1ZF5vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out):\n","        super().__init__()\n","\n","        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n","\n","        self.d_out = d_out\n","        self.head_dim = d_out // NUM_HEADS\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n","        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n","        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n","        self.out_proj = nn.Linear(d_out, d_out)\n","        self.dropout = nn.Dropout(DROP_RATE)\n","        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x)  # (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n","        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n","        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n","\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        attn_scores = queries @ keys.transpose(2, 3)\n","\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec)\n","\n","        return context_vec\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n","            GELU(),\n","            nn.Linear(4 * EMB_DIM, EMB_DIM),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=EMB_DIM,\n","            d_out=EMB_DIM)\n","\n","        self.ff = FeedForward()\n","        self.norm1 = LayerNorm(EMB_DIM)\n","        self.norm2 = LayerNorm(EMB_DIM)\n","        self.drop_shortcut = nn.Dropout(DROP_RATE)\n","\n","    def forward(self, x):\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut\n","\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut\n","\n","        return x\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n","        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n","        self.drop_emb = nn.Dropout(DROP_RATE)\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock() for _ in range(NUM_LAYERS)])\n","\n","        self.final_norm = LayerNorm(EMB_DIM)\n","        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"zFoIhDPsJfDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# gpu가 있으면 cuda else cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = \"cpu\"\n","print(device)\n","\n","torch.manual_seed(123)\n","model = GPTModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"],"metadata":{"id":"nmCOzCx-JlLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_seen, global_step = 0, -1\n","\n","losses = []\n","#모델을 에폭으로 학습시키는 것 시간 오래 걸림... gpu사자...\n","for epoch in range(1):\n","    model.train()  # Set model to training mode\n","\n","    epoch_loss = 0\n","    for input_batch, target_batch in train_loader:\n","        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","\n","        logits = model(input_batch)\n","        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","        epoch_loss += loss.item()\n","        loss.backward() # Calculate loss gradients\n","        optimizer.step() # Update model weights using loss gradients\n","        tokens_seen += input_batch.numel()\n","        global_step += 1\n","\n","        if global_step % 1000 == 0:\n","            print(f\"Tokens seen: {tokens_seen}\")\n","        # Optional evaluation step\n","\n","    avg_loss = epoch_loss / len(train_loader)\n","    losses.append(avg_loss)\n","    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n","    torch.save(model.state_dict(), \"model_\" + str(epoch + 1).zfill(3) + \".pth\")\n","\n","# 주의: 여기서는 편의상 모든 데이터를 train에 사용하였습니다.\n","#      ML에서는 일부 데이터를 validation에 사용하는 것이 일반적입니다."],"metadata":{"id":"77FsjUOoJnxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install matplotlib\n","import matplotlib.pyplot as plt\n","\n","plt.plot(losses)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.show()\n"],"metadata":{"id":"5Qv0LWz3KWAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파일로 저장했던 네트워크의 가중치들 읽어들이기\n","model.load_state_dict(torch.load(\"model_100.pth\", map_location=device, weights_only=True))\n","model.eval() # dropout을 사용하지 않음"],"metadata":{"id":"XvcX156yKWPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#학습 시킨 후 도비 이즈... 프리!\n","#실제 결과 (used, had, free, a , already, caught, !, heard, can, clearly...)\n","idx = tokenizer.encode(\"Dobby is\") # 토큰 id의 list\n","idx = torch.tensor(idx).unsqueeze(0).to(device)\n","\n","with torch.no_grad():\n","    logits = model(idx)\n","\n","logits = logits[:, -1, :]\n","\n","# 가장 확률이 높은 단어 10개 출력\n","top_logits, top_indices = torch.topk(logits, 10)\n","for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n","    print(f\"{p:.2f}\\t {i}\\t {tokenizer.decode([i])}\")\n","\n","# 가장 확률이 높은 단어 출력\n","idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n","flat = idx_next.squeeze(0) # 배치 차원 제거 torch.Size([1])\n","out = tokenizer.decode(flat.tolist()) # 텐서를 리스트로 바꿔서 디코드\n","print(out)"],"metadata":{"id":"vrkzsFLHKWcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        if top_k is not None:\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:\n","            break\n","\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx"],"metadata":{"id":"mSWVXUqmM2vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_context = input(\"Start context: \")\n","\n","#idx = tokenizer.encode(start_context, allowed_special={'<|endoftext|>'})\n","idx = tokenizer.encode(start_context)\n","idx = torch.tensor(idx).unsqueeze(0)\n","\n","context_size = model.pos_emb.weight.shape[0]\n","\n","for i in range(10):\n","\n","    token_ids = generate(\n","        model=model,\n","        idx=idx.to(device),\n","        #max_new_tokens=50,\n","        max_new_tokens=10,\n","        context_size= context_size,\n","        top_k=50,\n","        temperature=0.1\n","    )\n","\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    out = tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n","\n","    print(i, \":\", out)"],"metadata":{"id":"C0PjSGmQM2yB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JTBxan_lM20j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nRRxwr5WM23_"},"execution_count":null,"outputs":[]}]}